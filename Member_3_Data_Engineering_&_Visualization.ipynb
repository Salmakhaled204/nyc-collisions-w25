{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GY63KsQJdDL"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Python313/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# MEMBER 3 - Habiba Ayman — Data Cleaning & Validation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "U-qRGYaYJq9S",
        "outputId": "6713533c-83f7-4402-b6d1-5ec62cd352af"
      },
      "outputs": [],
      "source": [
        "# Step 0 — Load Raw Data\n",
        "crashes_url = \"https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=download\"\n",
        "persons_url = \"https://data.cityofnewyork.us/api/views/f55k-p6yu/rows.csv?accessType=download\"\n",
        "\n",
        "df_crashes = pd.read_csv(crashes_url, low_memory=False)\n",
        "df_persons = pd.read_csv(persons_url, low_memory=False)\n",
        "\n",
        "df_crashes.head()\n",
        "df_persons.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Mn8tu_JtkS",
        "outputId": "89afdd12-3b3e-4309-9870-0f9feece3bff"
      },
      "outputs": [],
      "source": [
        "# Step 1 — Backup & Normalize Columns\n",
        "df_crashes_raw = df_crashes.copy()\n",
        "df_persons_raw = df_persons.copy()\n",
        "\n",
        "def normalize_cols(df):\n",
        "    df = df.copy()\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "    return df\n",
        "\n",
        "df_crashes = normalize_cols(df_crashes)\n",
        "df_persons = normalize_cols(df_persons)\n",
        "\n",
        "print(\"Columns normalized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SseCBfUnJvuy",
        "outputId": "f37cceba-b6a3-4c63-ac62-69e99f6a0427"
      },
      "outputs": [],
      "source": [
        "# Step 2 — Crash Dataset Cleaning\n",
        "\n",
        "# Convert crash date and time\n",
        "df_crashes['crash_date'] = pd.to_datetime(df_crashes['crash_date'], errors='coerce')\n",
        "df_crashes['crash_time'] = pd.to_datetime(df_crashes['crash_time'], format='%H:%M', errors='coerce').dt.time\n",
        "\n",
        "# Create time features\n",
        "df_crashes['crash_hour'] = pd.to_datetime(df_crashes['crash_time'].astype(str), errors='coerce').dt.hour\n",
        "df_crashes['crash_year'] = df_crashes['crash_date'].dt.year\n",
        "df_crashes['crash_month'] = df_crashes['crash_date'].dt.to_period('M').astype(str)\n",
        "\n",
        "# Convert injury/fatality columns to numeric\n",
        "injury_cols = [c for c in df_crashes.columns if (\"injured\" in c or \"killed\" in c)]\n",
        "for col in injury_cols:\n",
        "    df_crashes[col] = pd.to_numeric(df_crashes[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "# Replace missing borough\n",
        "df_crashes['borough'] = df_crashes['borough'].fillna('Unknown')\n",
        "\n",
        "# Remove duplicate rows\n",
        "df_crashes = df_crashes.drop_duplicates()\n",
        "\n",
        "df_crashes.head()\n",
        "\n",
        "print(\"Crash data cleaned. Shape:\", df_crashes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AqMYrDDJxtC",
        "outputId": "40c1cf72-9021-4ba3-9071-e22ee7f61d2f"
      },
      "outputs": [],
      "source": [
        "# Step 3 — Persons Dataset Cleaning\n",
        "\n",
        "# Age cleaning\n",
        "df_persons['person_age'] = pd.to_numeric(df_persons['person_age'], errors='coerce')\n",
        "df_persons.loc[df_persons['person_age'] <= 0, 'person_age'] = np.nan\n",
        "df_persons.loc[df_persons['person_age'] > 110, 'person_age'] = np.nan\n",
        "\n",
        "# Impute missing ages\n",
        "global_median = df_persons['person_age'].median()\n",
        "median_by_type = df_persons.groupby('person_type')['person_age'].median()\n",
        "\n",
        "def impute_age(row):\n",
        "    if pd.notna(row['person_age']):\n",
        "        return row['person_age']\n",
        "    pt = row['person_type']\n",
        "    if pt in median_by_type and not np.isnan(median_by_type.loc[pt]):\n",
        "        return median_by_type.loc[pt]\n",
        "    return global_median\n",
        "\n",
        "df_persons['person_age_imputed'] = df_persons.apply(impute_age, axis=1)\n",
        "\n",
        "# Sex cleaning\n",
        "df_persons['person_sex'] = (\n",
        "    df_persons['person_sex'].astype(str).str.upper().replace({\n",
        "        'MALE':'M','FEMALE':'F','U':np.nan,'UNKNOWN':np.nan\n",
        "    })\n",
        ")\n",
        "df_persons['person_sex'] = df_persons['person_sex'].where(df_persons['person_sex'].isin(['M','F']), np.nan)\n",
        "\n",
        "# Person type formatting\n",
        "df_persons['person_type'] = df_persons['person_type'].astype(str).str.upper()\n",
        "\n",
        "# Injury category cleaning\n",
        "def clean_injury(x):\n",
        "    if pd.isna(x): return \"UNKNOWN\"\n",
        "    s = str(x).upper()\n",
        "    if \"KILLED\" in s: return \"KILLED\"\n",
        "    if \"INJUR\" in s: return \"INJURED\"\n",
        "    if \"NONE\" in s: return \"NONE\"\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "df_persons['person_injury_clean'] = df_persons['person_injury'].apply(clean_injury)\n",
        "\n",
        "# Remove duplicate rows\n",
        "df_persons = df_persons.drop_duplicates()\n",
        "\n",
        "print(\"Persons data cleaned. Shape:\", df_persons.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-yd642YJ0Il",
        "outputId": "00dfbdde-6100-4e5c-9f72-f30e737cb162"
      },
      "outputs": [],
      "source": [
        "# Step 4 — Save Cleaned Data\n",
        "df_crashes.to_csv(\"df_crashes_cleaned.csv\", index=False)\n",
        "df_persons.to_csv(\"df_persons_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"Cleaned datasets exported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Grlb8AqIJ2P6",
        "outputId": "54402246-ce33-4ab0-ef20-d76b9dbdf081"
      },
      "outputs": [],
      "source": [
        "# Step 5 — Validation Visualizations\n",
        "\n",
        "# Crash counts per borough\n",
        "plt.figure(figsize=(8,4))\n",
        "df_crashes['borough'].value_counts().plot(kind=\"bar\", color='skyblue')\n",
        "plt.title(\"Number of Crashes per Borough (Cleaned)\")\n",
        "plt.xlabel(\"Borough\")\n",
        "plt.ylabel(\"Crash Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Injury distributions\n",
        "plt.figure(figsize=(8,4))\n",
        "df_crashes['number_of_persons_injured'].hist(bins=30, color='orange')\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Distribution of Persons Injured (Cleaned)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "df_crashes['number_of_persons_killed'].hist(bins=30, color='red')\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Distribution of Persons Killed (Cleaned)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Age distribution\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.histplot(df_persons['person_age_imputed'], bins=40, kde=True, color='green')\n",
        "plt.title(\"Age Distribution (Cleaned & Imputed)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gender distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(y='person_sex', data=df_persons, palette='pastel')\n",
        "plt.title(\"Gender Distribution (Cleaned)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Person type distribution\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.countplot(y='person_type', data=df_persons, palette='muted')\n",
        "plt.title(\"Person Type Distribution (Cleaned)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndqrcDJcK69Q",
        "outputId": "cae6899a-adb5-4208-d934-d0f66e463255"
      },
      "outputs": [],
      "source": [
        "# Step 6 — Create Sample Datasets (1000 Rows for GitHub)\n",
        "\n",
        "# Take 1000 random rows from cleaned crashes\n",
        "sample_crashes = df_crashes.sample(n=1000, random_state=42)\n",
        "sample_crashes.to_csv(\"sample_crashes.csv\", index=False)\n",
        "\n",
        "# Take 1000 random rows from cleaned persons\n",
        "sample_persons = df_persons.sample(n=1000, random_state=42)\n",
        "sample_persons.to_csv(\"sample_persons.csv\", index=False)\n",
        "\n",
        "print(\"Sample files created: sample_crashes.csv, sample_persons.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z1f9ebxfJ4Vz",
        "outputId": "ad639962-304d-4b47-d4dc-bc192a4a53f6"
      },
      "outputs": [],
      "source": [
        "# Missing Values Bar Plot\n",
        "def plot_missing_percentages(df, title):\n",
        "    missing_percent = df.isnull().sum() * 100 / len(df)\n",
        "    missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
        "\n",
        "    if not missing_percent.empty:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=missing_percent.values, y=missing_percent.index,\n",
        "                    hue=missing_percent.index, palette='viridis', legend=False)\n",
        "        plt.title(f\"Percentage of Missing Values — {title}\")\n",
        "        plt.xlabel(\"Percent Missing (%)\")\n",
        "        plt.ylabel(\"Column Name\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No missing values found in {title}.\")\n",
        "\n",
        "plot_missing_percentages(df_crashes, \"Crashes (Cleaned)\")\n",
        "plot_missing_percentages(df_persons, \"Persons (Cleaned)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "taBYf5g9J9MZ",
        "outputId": "a81205ad-795c-4fcd-ab10-9b2b8a62d8aa"
      },
      "outputs": [],
      "source": [
        "# Age Boxplot\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(x='person_age_imputed', data=df_persons, color='lightblue')\n",
        "plt.title(\"Boxplot of Person Age (Cleaned & Imputed)\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXm84o60J92D",
        "outputId": "6bd5c3f0-2177-481a-a1e1-0836600316e9"
      },
      "outputs": [],
      "source": [
        "# Check persons linked to missing crashes\n",
        "missing_crash_ids = df_persons[~df_persons['collision_id'].isin(df_crashes['collision_id'])]\n",
        "print(\"Number of persons linked to missing crash IDs:\", missing_crash_ids.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcpchrQ4KDqh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltDEpdillZ25"
      },
      "source": [
        "# **Member 3 — Data Cleaning, Validation & Standardization Summary**\n",
        "## 1. Objectives & Scope\n",
        "\n",
        "Member 3 focused on transforming the raw crashes and persons datasets into a clean, validated, standardized form suitable for merging and deeper analysis. This stage ensured the data is accurate, consistent, and free of structural errors. Key objectives included:\n",
        "\n",
        "- Standardizing column names and formats\n",
        "\n",
        "- Cleaning timestamps and generating usable time features\n",
        "\n",
        "- Handling missing values, invalid entries, and inconsistencies\n",
        "\n",
        "- Standardizing demographic fields (age, sex, injury category, person type)\n",
        "\n",
        "- Removing duplicates and validating record integrity\n",
        "\n",
        "- Producing validation graphs to confirm successful cleaning\n",
        "\n",
        "- Delivering cleaned datasets ready for Member 4\n",
        "\n",
        "# 2. Crash Dataset Cleaning\n",
        "\n",
        "Major steps performed on the crashes dataset included:\n",
        "\n",
        "- Converting crash_date to proper datetime format\n",
        "\n",
        "- Parsing crash_time and extracting crash_hour, crash_month, and crash_year\n",
        "\n",
        "- Ensuring all injury/fatality fields were numeric and filling missing values with zeros\n",
        "\n",
        "- Standardizing borough data and replacing missing entries with \"Unknown\"\n",
        "\n",
        "- Removing duplicate crash records to ensure dataset integrity\n",
        "\n",
        "- Verifying cleaned distributions using bar charts and histograms\n",
        "\n",
        "# 3. Persons Dataset Cleaning\n",
        "\n",
        "Cleaning the persons dataset required extra validation due to demographic inconsistencies. Key tasks included:\n",
        "\n",
        "- Converting age to numeric, removing invalid values (<0 or >110), and imputing missing ages using medians\n",
        "\n",
        "- Standardizing sex values to M and F, treating unknowns as missing\n",
        "\n",
        "- Normalizing person type to consistent uppercase categories\n",
        "\n",
        "Cleaning injury categories to the standardized labels:\n",
        "\n",
        "1- NONE\n",
        "\n",
        "2- INJURED\n",
        "\n",
        "3- KILLED\n",
        "\n",
        "4- UNKNOWN\n",
        "\n",
        "5- Dropping duplicate entries\n",
        "\n",
        "6- Creating validation plots for gender, age, injury distribution, and person type\n",
        "\n",
        "# 4. Post-Cleaning Validation & Quality Checks\n",
        "\n",
        "To verify the cleaning process, multiple diagnostic visualizations were produced:\n",
        "\n",
        "- Borough-wise crash counts\n",
        "\n",
        "- Injury/fatality histograms on the cleaned dataset\n",
        "\n",
        "- Age distribution (raw vs. imputed)\n",
        "\n",
        "- Gender and person type distribution\n",
        "\n",
        "- Missing-values barplots for both datasets\n",
        "\n",
        "- Age boxplot to check remaining outliers\n",
        "\n",
        "Collision-ID integrity check confirming:\n",
        "- 0 persons referenced a missing crash\n",
        "\n",
        "These checks confirmed that both datasets are standardized, consistent, and ready for merging.\n",
        "\n",
        "# 5. Handover to Member 4\n",
        "\n",
        "Member 3 delivered:\n",
        "\n",
        "- df_crashes_cleaned.csv\n",
        "\n",
        "- df_persons_cleaned.csv\n",
        "\n",
        "- Full documentation of cleaning decisions\n",
        "\n",
        "- All validation graphs\n",
        "\n",
        "- Confirmation that collision IDs align perfectly between datasets\n",
        "\n",
        "Member 4 can now safely merge datasets, engineer features, and perform final analysis using the cleaned master files."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
